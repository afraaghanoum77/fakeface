import os
import cv2
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import img_to_array, load_img
from tensorflow.keras.utils import to_categorical
from deepface import DeepFace
import random
from time import sleep
from datetime import datetime
from termcolor import colored
import logging
from rich.console import Console
from rich.table import Table
from rich.progress import track, Progress
from fpdf import FPDF
import plotly.express as px
from threading import Thread
from multiprocessing import Pool
import pandas as pd
import tensorflow as tf


# Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« (Logging)
logging.basicConfig(filename='app.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Rich
console = Console()

# Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
LANGUAGES = {
    "en": {
        "menu_title": "ğŸŒŸ Face and Image Analysis System ğŸŒŸ",
        "camera_analysis": "ğŸ¥ Start Face Analysis from Camera",
        "image_analysis": "ğŸ“· Analyze Images from Dataset",
        "exit": "ğŸšª Exit",
        "select_option": "Select an option (1/2/3): ",
        "invalid_choice": "Invalid choice. Please enter 1, 2, or 3.",
        "starting_camera": "Starting camera...",
        "camera_failed": "Failed to open camera.",
        "camera_active": "Camera is active. Press 'q' to quit.",
        "analysis_failed": "Analysis failed: {}",
        "total_images": "ğŸ–¼ï¸ Total images analyzed: {}",
        "real_images": "âœ… Real images: {}",
        "fake_images": "âŒ Fake images: {}",
        "emotion_distribution": "===== Emotion Distribution =====",
        "report_generated": "Report generated: {}",
    },
    "tr": {
        "menu_title": "ğŸŒŸ YÃ¼z ve GÃ¶rÃ¼ntÃ¼ Analizi Sistemi ğŸŒŸ",
        "camera_analysis": "ğŸ¥ Kameradan YÃ¼z Analizi BaÅŸlat",
        "image_analysis": "ğŸ“· Veri Setinden GÃ¶rÃ¼ntÃ¼ Analizi Yap",
        "exit": "âŒ Ã‡Ä±kÄ±ÅŸ",
        "select_option": "SeÃ§iminizi yapÄ±n (1/2/3): ",
        "invalid_choice": "GeÃ§ersiz seÃ§im. LÃ¼tfen 1, 2 veya 3 girin.",
        "starting_camera": "KamerayÄ± baÅŸlatÄ±yor...",
        "camera_failed": "Kamera aÃ§Ä±lamadÄ±.",
        "camera_active": "Kamera aktif. Ã‡Ä±kmak iÃ§in 'q' tuÅŸuna basÄ±n.",
        "analysis_failed": "Analiz baÅŸarÄ±sÄ±z: {}",
        "total_images": "ğŸ–¼ï¸ Analiz edilen toplam gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {}",
        "real_images": "âœ… GerÃ§ek gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {}",
        "fake_images": "âŒ Sahte gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {}",
        "emotion_distribution": "===== DuygularÄ±n DaÄŸÄ±lÄ±mÄ± =====",
        "report_generated": "Rapor oluÅŸturuldu: {}",
    }
}

# Ø§Ù„Ù„ØºØ© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
LANGUAGE = "tr"

def t(key):
    """ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©."""
    return LANGUAGES[LANGUAGE].get(key, key)

# ØªØ­Ø³ÙŠÙ† ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
def print_menu():
    """Ø·Ø¨Ø§Ø¹Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª."""
    console.print("=" * 50, style="bold cyan")
    console.print(t("menu_title"), style="bold cyan")
    console.print("=" * 50, style="bold cyan")
    console.print(f"1. {t('camera_analysis')}", style="green")
    console.print(f"2. {t('image_analysis')}", style="yellow")
    console.print(f"3. {t('exit')}", style="red")
    console.print("=" * 50, style="bold cyan")

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
def user_input():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…."""
    print_menu()
    secim = console.input(f"[blue]{t('select_option')}[/blue]")
    
    if secim == "1":
        return "camera"
    elif secim == "2":
        return "image_analysis"
    elif secim == "3":
        return "exit"
    else:
        console.print(f"[red]{t('invalid_choice')}[/red]")
        return user_input()

# ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
def camera_analysis():
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ù…Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§."""
    console.print(f"[cyan]{t('starting_camera')}[/cyan]")
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        console.print(f"[red]{t('camera_failed')}[/red]")
        logging.error(t("camera_failed"))
    else:
        console.print(f"[green]{t('camera_active')}[/green]")
        emotions_count = {'happy': 0, 'sad': 0, 'angry': 0, 'surprise': 0, 'fear': 0, 'neutral': 0}
        
        while True:
            ret, frame = cap.read()
            if not ret:
                console.print("[red]Kamera gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±namÄ±yor.[/red]")
                logging.error("Kamera gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±namÄ±yor.")
                break
            
            try:
                analysis = DeepFace.analyze(frame, actions=['emotion', 'age', 'gender', 'race'], enforce_detection=False)
                emotion = analysis[0]['dominant_emotion']
                age = analysis[0]['age']
                gender = analysis[0]['dominant_gender']
                race = analysis[0]['dominant_race']
                
                emotions_count[emotion] += 1
                
                cv2.putText(frame, f"Duygu: {emotion}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(frame, f"YaÅŸ: {age}", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(frame, f"Cinsiyet: {gender}", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(frame, f"Irk: {race}", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            except Exception as e:
                console.print(f"[red]{t('analysis_failed').format(e)}[/red]")
                logging.error(f"Analiz baÅŸarÄ±sÄ±z: {e}")
            
            cv2.imshow("Kamera GÃ¶rÃ¼ntÃ¼sÃ¼ - Analiz", frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        generate_report(emotions_count, source="camera")

# ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±
def image_analysis():
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    console.print("[cyan]GÃ¶rÃ¼ntÃ¼ analiz kodunu Ã§alÄ±ÅŸtÄ±rÄ±yor...[/cyan]")
    
    input_shape = (170, 170, 3)
    data_dir = r'.\Data'  # Veri klasÃ¶rÃ¼nÃ¼n yolu

    real_data = [f for f in os.listdir(os.path.join(data_dir, 'training_real')) if f.endswith('.jpg')]
    fake_data = [f for f in os.listdir(os.path.join(data_dir, 'training_fake')) if f.endswith('.jpg')]

    X = []
    Y = []

    for img in track(real_data, description="[green]GerÃ§ek gÃ¶rÃ¼ntÃ¼ler yÃ¼kleniyor...[/green]"):
        img_path = os.path.join(data_dir, 'training_real', img)
        img = load_img(img_path, target_size=(170, 170))
        X.append(img_to_array(img) / 255.0)
        Y.append(1)

    for img in track(fake_data, description="[red]Sahte gÃ¶rÃ¼ntÃ¼ler yÃ¼kleniyor...[/red]"):
        img_path = os.path.join(data_dir, 'training_fake', img)
        img = load_img(img_path, target_size=(170, 170))
        X.append(img_to_array(img) / 255.0)
        Y.append(0)

    X = np.array(X)
    Y = to_categorical(Y, 2)

    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=5)

    emotions_count = {'happy': 0, 'sad': 0, 'angry': 0, 'surprise': 0, 'fear': 0, 'neutral': 0}

    num_samples = 5 
    random_indices = random.sample(range(len(X_val)), num_samples)

    real_count = 0
    fake_count = 0

    for i in random_indices:
        plt.figure(figsize=(8, 6))
        plt.imshow(X_val[i])
        plt.axis('off')
        
        label = "GerÃ§ek" if np.argmax(Y_val[i]) == 1 else "Sahte"
        plt.title(f"{label}", fontsize=16, color='darkgreen')

        if label == "GerÃ§ek":
            real_count += 1
            try:
                rgb_image = cv2.cvtColor((X_val[i] * 255).astype("uint8"), cv2.COLOR_BGR2RGB)
                analysis = DeepFace.analyze(rgb_image, actions=['emotion'], enforce_detection=False)
                if isinstance(analysis, list):
                    analysis = analysis[0]
                emotion = analysis.get('dominant_emotion', "Bilinmiyor")
                emotions_count[emotion] += 1
            except Exception as e:
                emotion = "Bilinmiyor"
            
            plt.gca().text(0.5, -0.1, f"Duygular: {emotion}", ha='center', va='top', fontsize=14, color='blue', transform=plt.gca().transAxes)
        else:
            fake_count += 1
            plt.gca().text(0.5, -0.1, "Duygular: -", ha='center', va='top', fontsize=14, color='gray', transform=plt.gca().transAxes)
        
        plt.show()
        sleep(1)
    console.print(f"\n[bold cyan]{t('total_images').format(num_samples)}[/bold cyan]")
    console.print(f"âœ… [green]{t('real_images').format(real_count)}[/green]")
    console.print(f"âŒ [red]{t('fake_images').format(fake_count)}[/red]")

    console.print(f"\n[bold cyan]{t('emotion_distribution')}[/bold cyan]")
    for emotion, count in emotions_count.items():
        console.print(f"{emotion.capitalize()} duygusu sayÄ±sÄ±: {count}")

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    generate_report(emotions_count, source="image_analysis", real_images_count=real_count, fake_images_count=fake_count)

# Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± PDF
def generate_pdf_report(emotions_count, source, real_images_count=None, fake_images_count=None):
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± PDF Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„."""
    pdf = FPDF()
    pdf.add_page()
    
    # Ø¥Ø¶Ø§ÙØ© Ø®Ø· ÙŠØ¯Ø¹Ù… Unicode (Ù…Ø«Ù„ DejaVuSans)
    pdf.add_font('Monomakh', '', './Fonts/Monomakh/Monomakh-Regular.ttf', uni=True)
    pdf.set_font('Monomakh', size=12)
    
    pdf.cell(200, 10, txt="===== Analiz Raporu =====", ln=True, align='C')
    pdf.cell(200, 10, txt=f"Rapor Tarihi: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", ln=True, align='C')
    
    if source == "camera":
        pdf.cell(200, 10, txt="1. Kameradan YÃ¼z Analizi:", ln=True)
        pdf.cell(200, 10, txt="   - Duygu, yaÅŸ ve cinsiyet analizi yapÄ±ldÄ±.", ln=True)
    elif source == "image_analysis":
        pdf.cell(200, 10, txt="1. Veri Setinden GÃ¶rÃ¼ntÃ¼ Analizi:", ln=True)
        pdf.cell(200, 10, txt=f"   - Analiz edilen toplam gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {real_images_count + fake_images_count}", ln=True)
        pdf.cell(200, 10, txt=f"   - GerÃ§ek gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {real_images_count}", ln=True)
        pdf.cell(200, 10, txt=f"   - Sahte gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {fake_images_count}", ln=True)
        pdf.cell(200, 10, txt="______________________________________________", ln=True)
        pdf.cell(200, 10, txt="\n2. DuygularÄ±n DaÄŸÄ±lÄ±mÄ±:", ln=True)
    for emotion, count in emotions_count.items():
        pdf.cell(200, 10, txt=f"   - {emotion.capitalize()}: {count}", ln=True)
    
    report_dir = "Reports"
    if not os.path.exists(report_dir):
        os.makedirs(report_dir)
    
    report_name = f"report_{datetime.now().strftime('%Y-%m-%d')}.pdf"
    report_path = os.path.join(report_dir, report_name)
    pdf.output(report_path)
    console.print(f"[green]{t('report_generated').format(report_path)}[/green]")

# Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„
def generate_report(emotions_count, source, real_images_count=None, fake_images_count=None):
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„."""
    generate_pdf_report(emotions_count, source, real_images_count, fake_images_count)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Plotly
    if source == "image_analysis":
         # Ù…Ø®Ø·Ø· ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        fig1 = px.bar(x=list(emotions_count.keys()), y=list(emotions_count.values()), 
             labels={'x': 'Duygu', 'y': 'SayÄ±'}, 
             title="DuygularÄ±n DaÄŸÄ±lÄ±mÄ±", 
             color=list(emotions_count.keys()),
             text=list(emotions_count.values())) 
        fig1.update_traces(textposition='outside')
        fig1.show()
        # Ù…Ø®Ø·Ø· ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆØ§Ù„Ù…Ø²ÙŠÙØ©
        fig2 = px.pie(names=['GerÃ§ek GÃ¶rÃ¼ntÃ¼ler', 'Sahte GÃ¶rÃ¼ntÃ¼ler'], 
                     values=[real_images_count, fake_images_count], 
                     title="GÃ¶rÃ¼ntÃ¼ DaÄŸÄ±lÄ±mÄ±: GerÃ§ek vs Sahte",
                     color=['GerÃ§ek GÃ¶rÃ¼ntÃ¼ler', 'Sahte GÃ¶rÃ¼ntÃ¼ler'],
                     color_discrete_map={'GerÃ§ek GÃ¶rÃ¼ntÃ¼ler': 'green', 'Sahte GÃ¶rÃ¼ntÃ¼ler': 'red'})
        fig2.show()

# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…."""
    while True:
        choice = user_input()
        
        if choice == "camera":
            camera_analysis()
        elif choice == "image_analysis":
            image_analysis()
        elif choice == "exit":
            console.print("[red]Ã‡Ä±kÄ±yor... TeÅŸekkÃ¼r ederiz! ğŸ‘‹[/red]", style="bold")
            break  

if __name__ == "__main__":
    main()